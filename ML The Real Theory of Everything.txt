Have you ever watch "The Theory of Everything"? It's a biography movie of Stephen Hawking and his journey to discover a formula which could explain every single phenomena in the universe. If you haven't watch it, watch it, it's a cool movie. Don't worry this movie is not as nerdy as it sounds, so even if you are not nerd enough to understand Hawkin's books, you still able to enjoy the movie. When I was wathching this movie, my imagination was running wild and somehow I found something interesting which is the similiraty of intuition between Hawkin's "Theory of Everything" and Machine Learning. In this article I would like to try to explain the basic of machine learning using "The Theory of Everything" perspective.
In the movie, Hawkin believes that the universe work based on certain rule or formula that human doesn't know yet due to limitation of obervable phenomena and every known formula is just small part and could be used to approximate "The theory of Everything". From my perspective, this believe could explain how machine learning actually work ideally which is by approximating formula (the theory of everything) that could explain every single output for every input (universe) by deriving formula (small part of formula) from our data set (observable phenomena). 
So for example we have data set in form of stock price for certain company in the last quarter. Some people might think that the stockprice is random and it's humanly impossible to find a pattern to explain and predict it even with the help from the best economist on earth. But, if we put Hawkin's perspective in stockprice analysis, we would never say that there is no pattern, or random, but we would say that there is pattern but it is humanly imposible to observe that pattern. Therefore this is the time for human to step back and let machine do their job which is to find that humanly unobservable pattern. In machine learning the machine find the pattern through iteration of trial and error called training. To put it simply, training is just a process to match every posible pattern with dataset and stop when the pattern match all data in the data set.
At this point, you might wonder how can the pattern (the graph) in the first iteration change into the pattern in the second and so on. Well, on the picture above the pattern change randomly each iteration, but if the pattern change randomly, even though it takes less computational resources, it is not a reliable practice because it might takes significant amout of time just to find best-match pattern for simple data. If in his research, Stephen Hawkin just make random formula and test it to every phenomena, even if he lives for hundreds years he might not find the formula he's looking for. Fortunately, our Stephen Hawkin is smart and time efficient, so in his research he follow certain guidance and rule to extrapolate the formula. This guidance and rule is called update rule in machine learning The implementation of update rule will ensure that every iteration produce pattern which match our data set better than the previous one by minimizing convergence error.  
What is convergence error? Well, it is just fancy way of saying accuracy of our pattern, compared with data set. If convergence error is big it means that our model failed to explain most of dataset and if it's small it means that our model capable to explain most of dataset. It's worth to be noted that just because the convergence error is very small or even 0 that doesn't mean that our model capable to 100% predict acurately future stockprice. Small convergence errors only means that our model will acurately predict the past stockprice given any input INSIDE the data set. So if we give an input, OUTSIDE the dataset our model might not be able to predict it accurately. So what's the point of machine learning if it can't predict future stockprice? Let's take a step back and put "Theory of Everything" perspective in the context. Remember that indeed, all currently known formula can't explain all phenomena like "Theory of Everything", but since we are limited by observable phenomena and can't formulate the theory of everything yet, known formula is enough and can be usefull in real world even though not always capable to explain some anomaly. In our machine learning model, even though it can only accurately predict the data inside the dataset it's enough because it is approximation of stockprices' "the theory of everything". If we want to increase the accuracy of our model to predict based on input outside the data set, what we need to do is increase the quality and quantity of the data set used in training process so the machine can get closer approximation to the stockprices' "the theory of everything". Treat dataset like physician treat observable phenomena. Just like, the more physician observe a phenomena the more they understand universe and able to derive certain formula from that understanding, the more dataset we use in training the more our machine can understand and create a model based on our data set.
